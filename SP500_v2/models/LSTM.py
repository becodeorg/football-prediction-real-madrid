"""
LSTM Neural Network Predictive Model for S&P 500 Trading using PyTorch
Uses CSV data generated by the dataset module for training and prediction with deep learning
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
import json

warnings.filterwarnings('ignore')

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import classification_report, confusion_matrix
    PYTORCH_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Some dependencies not available: {e}")
    print("   Install with: pip install torch scikit-learn")
    PYTORCH_AVAILABLE = False

class LSTMModel(nn.Module):
    """PyTorch LSTM Model for time series prediction"""
    def __init__(self, input_size, hidden_size=50, num_layers=2, output_size=1, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # Decode the hidden state of the last time step
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        return out

class SP500LSTMPredictor:
    """
    LSTM-based predictive model for S&P 500 price movements using PyTorch
    Loads data from CSV files and provides comprehensive deep learning pipeline
    """
    def __init__(self, csv_dir: str = None, model_save_dir: str = None, sequence_length: int = 20):
        if csv_dir is None:
            csv_dir = os.path.join(parent_dir, 'data', 'csv')
        if model_save_dir is None:
            model_save_dir = os.path.join(parent_dir, 'models', 'saved_models')
        self.csv_dir = csv_dir
        self.model_save_dir = model_save_dir
        self.sequence_length = sequence_length
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        # Data storage
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.X_train_seq = None
        self.X_test_seq = None
        self.y_train_seq = None
        self.y_test_seq = None
        self.feature_names = None
        self.target_type = None
        
        # Model storage
        self.models = {}
        self.scalers = {}
        self.model_metrics = {}
        self.feature_importance = {}
        
        # PyTorch device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

    def prepare_sequences(self, data: np.ndarray, target: np.ndarray, sequence_length: int):
        """Prepare sequential data for LSTM input"""
        X, y = [], []
        for i in range(sequence_length, len(data)):
            X.append(data[i-sequence_length:i])
            y.append(target[i])
        return np.array(X), np.array(y)

    def load_data(self, target_type: str = 'regression') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Load and prepare data for LSTM training"""
        train_path = os.path.join(self.csv_dir, 'GSPC_train_2020-01-01_20250807.csv')
        test_path = os.path.join(self.csv_dir, 'GSPC_test_2020-01-01_20250807.csv')
        
        train_df = pd.read_csv(train_path, index_col=0)
        test_df = pd.read_csv(test_path, index_col=0)
        
        # Use the single 'target' column for both regression and classification
        target_col = 'target'
        self.feature_names = [col for col in train_df.columns if col not in ['date', target_col]]
        self.X_train = train_df[self.feature_names]
        self.X_test = test_df[self.feature_names]
        
        if target_type == 'regression':
            # Use the continuous target values for regression
            self.y_train = train_df[target_col]
            self.y_test = test_df[target_col]
        else:
            # Create binary classification targets (1 for positive returns, 0 for negative)
            self.y_train = (train_df[target_col] > 0).astype(int)
            self.y_test = (test_df[target_col] > 0).astype(int)
            
        # Scale features for LSTM
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        
        # Store scaler for later use
        self.scalers['feature_scaler'] = scaler
        
        # Prepare sequences for LSTM
        if len(X_train_scaled) > self.sequence_length:
            self.X_train_seq, self.y_train_seq = self.prepare_sequences(
                X_train_scaled, self.y_train.values, self.sequence_length)
        else:
            # If not enough data, use smaller sequence length
            seq_len = min(10, len(X_train_scaled) - 1)
            self.X_train_seq, self.y_train_seq = self.prepare_sequences(
                X_train_scaled, self.y_train.values, seq_len)
            
        if len(X_test_scaled) > self.sequence_length:
            self.X_test_seq, self.y_test_seq = self.prepare_sequences(
                X_test_scaled, self.y_test.values, self.sequence_length)
        else:
            # If not enough data, use smaller sequence length
            seq_len = min(10, len(X_test_scaled) - 1)
            self.X_test_seq, self.y_test_seq = self.prepare_sequences(
                X_test_scaled, self.y_test.values, seq_len)
        
        self.target_type = target_type
        return self.X_train_seq, self.X_test_seq, self.y_train_seq, self.y_test_seq

    def train_model(self, model_name: str = 'default', epochs: int = 50, batch_size: int = 32, 
                   learning_rate: float = 0.001, hidden_size: int = 50, num_layers: int = 2):
        """Train LSTM model using PyTorch"""
        if self.X_train_seq is None or self.y_train_seq is None:
            raise ValueError('Data not loaded. Call load_data() first!')
        
        # Convert to PyTorch tensors
        X_train_tensor = torch.FloatTensor(self.X_train_seq).to(self.device)
        y_train_tensor = torch.FloatTensor(self.y_train_seq).to(self.device)
        
        # Reshape y for proper dimensions
        if len(y_train_tensor.shape) == 1:
            y_train_tensor = y_train_tensor.unsqueeze(1)
        
        # Create dataset and dataloader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # Initialize model
        input_size = self.X_train_seq.shape[2]
        model = LSTMModel(input_size=input_size, hidden_size=hidden_size, 
                         num_layers=num_layers, output_size=1).to(self.device)
        
        # Loss function and optimizer
        if self.target_type == 'classification':
            criterion = nn.BCEWithLogitsLoss()
        else:
            criterion = nn.MSELoss()
        
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        # Training loop
        model.train()
        training_losses = []
        
        for epoch in range(epochs):
            epoch_losses = []
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_losses.append(loss.item())
            
            avg_loss = np.mean(epoch_losses)
            training_losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.4f}')
        
        # Store model
        self.models[model_name] = model
        
        # Save model
        model_dir = os.path.join(self.model_save_dir, f"lstm_{self.target_type}_{model_name}")
        os.makedirs(model_dir, exist_ok=True)
        
        torch.save(model.state_dict(), os.path.join(model_dir, 'lstm_model.pth'))
        joblib.dump(self.scalers['feature_scaler'], os.path.join(model_dir, 'feature_scaler.joblib'))
        
        # Save training history
        history_dict = {'training_loss': training_losses}
        with open(os.path.join(model_dir, 'training_history.json'), 'w') as f:
            json.dump(history_dict, f)
        
        # Calculate feature importance
        self._calculate_feature_importance(model, model_name)
        
        print(f"Model '{model_name}' trained successfully!")

    def _calculate_feature_importance(self, model, model_name: str):
        """Calculate approximate feature importance using input gradients"""
        try:
            model.eval()
            
            # Use a sample from training data
            sample_size = min(100, len(self.X_train_seq))
            sample_input = torch.FloatTensor(self.X_train_seq[:sample_size]).to(self.device)
            sample_input.requires_grad_()
            
            # Forward pass
            outputs = model(sample_input)
            loss = torch.mean(outputs)
            
            # Backward pass to get gradients
            loss.backward()
            
            # Calculate feature importance as mean absolute gradient across samples and time steps
            gradients = sample_input.grad.detach().cpu().numpy()
            importance = np.mean(np.abs(gradients), axis=(0, 1))
            
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            # Save feature importance
            model_dir = os.path.join(self.model_save_dir, f"lstm_{self.target_type}_{model_name}")
            importance_df.to_csv(os.path.join(model_dir, 'feature_importance.csv'), index=False)
            self.feature_importance[model_name] = importance_df
            
        except Exception as e:
            print(f"Could not calculate feature importance: {e}")
            # Create dummy importance
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': np.random.random(len(self.feature_names))
            }).sort_values('importance', ascending=False)
            self.feature_importance[model_name] = importance_df

    def evaluate_model(self, model_name: str = 'default') -> Dict:
        """Evaluate trained model on test data"""
        model = self.models.get(model_name)
        if model is None:
            raise ValueError(f'Model {model_name} not found!')
        
        model.eval()
        
        # Convert test data to tensors
        X_test_tensor = torch.FloatTensor(self.X_test_seq).to(self.device)
        
        with torch.no_grad():
            predictions = model(X_test_tensor)
            y_pred = predictions.cpu().numpy().flatten()
        
        metrics = {}
        
        if self.target_type == 'regression':
            metrics['test_rmse'] = np.sqrt(mean_squared_error(self.y_test_seq, y_pred))
            metrics['test_mae'] = mean_absolute_error(self.y_test_seq, y_pred)
            metrics['test_r2'] = r2_score(self.y_test_seq, y_pred)
            # Add direction accuracy for regression
            direction_correct = ((self.y_test_seq > 0) == (y_pred > 0)).sum()
            metrics['test_direction_accuracy'] = direction_correct / len(self.y_test_seq)
        else:
            # Apply sigmoid for classification probabilities
            y_pred_prob = torch.sigmoid(torch.FloatTensor(y_pred)).numpy()
            y_pred_class = (y_pred_prob > 0.5).astype(int)
            
            metrics['test_accuracy'] = accuracy_score(self.y_test_seq, y_pred_class)
            metrics['test_precision'] = precision_score(self.y_test_seq, y_pred_class)
            metrics['test_recall'] = recall_score(self.y_test_seq, y_pred_class)
            metrics['test_f1_score'] = f1_score(self.y_test_seq, y_pred_class)
            cm = confusion_matrix(self.y_test_seq, y_pred_class)
            metrics['confusion_matrix'] = cm.flatten().tolist()
        
        self.model_metrics[model_name] = metrics
        return metrics

    def get_model_summary(self, model_name: str = 'default') -> Dict:
        """Get comprehensive model summary"""
        return {
            'model_name': model_name,
            'task_type': self.target_type,
            'performance_metrics': self.model_metrics.get(model_name, {}),
            'feature_importance': self.feature_importance.get(model_name, pd.DataFrame()),
            'data_info': {
                'features': len(self.feature_names) if self.feature_names else 0,
                'train_samples': len(self.X_train_seq) if self.X_train_seq is not None else 0,
                'test_samples': len(self.X_test_seq) if self.X_test_seq is not None else 0,
                'sequence_length': self.sequence_length
            }
        }

    def _render_overview_tab(self, models: List[str]) -> None:
        """Render the overview tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ğŸ§  LSTM Neural Network - Dataset & Models Overview")
        
        # Dataset info
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Models", len(models))
            st.metric("Training Sequences", len(self.X_train_seq) if self.X_train_seq is not None else 0)
        
        with col2:
            st.metric("Test Sequences", len(self.X_test_seq) if self.X_test_seq is not None else 0)
            st.metric("Features", len(self.feature_names) if self.feature_names else 0)
        
        with col3:
            st.metric("Sequence Length", self.sequence_length)
            st.metric("Device", str(self.device).upper())
            if self.X_train_seq is not None:
                st.write(f"**Input Shape:** {self.X_train_seq.shape}")
            
        # Models summary
        st.subheader("ğŸ”— Trained LSTM Models")
        models_data = []
        for model_name in models:
            if model_name in self.model_metrics:
                summary = self.get_model_summary(model_name)
                models_data.append({
                    'Model': model_name,
                    'Type': summary['task_type'].title(),
                    'Features': summary['data_info']['features'],
                    'Status': 'âœ… Trained & Evaluated',
                    'Framework': 'PyTorch'
                })
            else:
                models_data.append({
                    'Model': model_name,
                    'Type': 'Unknown',
                    'Features': 'N/A',
                    'Status': 'âš ï¸ Trained Only',
                    'Framework': 'PyTorch'
                })
        
        if models_data:
            st.dataframe(pd.DataFrame(models_data), use_container_width=True)
        else:
            st.info("No models trained yet. Train a model first!")
    
    def _render_performance_tab(self, models: List[str]) -> None:
        """Render the performance metrics tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ğŸ¯ LSTM Performance Metrics")
        
        if not models:
            st.error("No models available. Train a model first!")
            return
            
        selected_model = st.selectbox("Select LSTM Model:", models)
        
        if selected_model not in self.model_metrics:
            st.error(f"Model '{selected_model}' not evaluated yet!")
            return
        
        summary = self.get_model_summary(selected_model)
        metrics = summary['performance_metrics']
        
        # Model info
        st.subheader("ğŸ“Š Model Information")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.write(f"**Framework:** PyTorch")
            st.write(f"**Task Type:** {summary['task_type'].title()}")
        
        with col2:
            st.write(f"**Sequence Length:** {summary['data_info']['sequence_length']}")
            st.write(f"**Features:** {summary['data_info']['features']}")
        
        with col3:
            st.write(f"**Train Samples:** {summary['data_info']['train_samples']}")
            st.write(f"**Test Samples:** {summary['data_info']['test_samples']}")
        
        # Performance metrics display
        st.subheader("ğŸ“ˆ Performance Metrics")
        
        if summary['task_type'] == 'regression':
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Test RMSE", f"{metrics['test_rmse']:.6f}")
            
            with col2:
                st.metric("Test MAE", f"{metrics['test_mae']:.6f}")
            
            with col3:
                st.metric("Test RÂ²", f"{metrics['test_r2']:.6f}")
            
            with col4:
                if 'test_direction_accuracy' in metrics:
                    st.metric("Direction Accuracy", f"{metrics['test_direction_accuracy']:.4f}")
            
            # Performance summary
            st.subheader("ğŸ“‹ LSTM Regression Summary")
            summary_data = {
                'Metric': ['RMSE', 'MAE', 'RÂ²', 'Direction Accuracy'],
                'Value': [
                    f"{metrics['test_rmse']:.6f}",
                    f"{metrics['test_mae']:.6f}", 
                    f"{metrics['test_r2']:.6f}",
                    f"{metrics.get('test_direction_accuracy', 0):.4f}"
                ]
            }
            st.dataframe(pd.DataFrame(summary_data), use_container_width=True)
        
        else:  # Classification
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Test Accuracy", f"{metrics['test_accuracy']:.4f}")
            
            with col2:
                st.metric("Test Precision", f"{metrics['test_precision']:.4f}")
            
            with col3:
                st.metric("Test Recall", f"{metrics['test_recall']:.4f}")
            
            with col4:
                st.metric("Test F1-Score", f"{metrics['test_f1_score']:.4f}")
            
            # Confusion Matrix
            st.subheader("ï¿½ Confusion Matrix")
            cm = metrics.get('confusion_matrix', [0, 0, 0, 0])
            if isinstance(cm, list) and len(cm) == 4:
                cm_df = pd.DataFrame([
                    ['True Negative', cm[0]],
                    ['False Positive', cm[1]], 
                    ['False Negative', cm[2]],
                    ['True Positive', cm[3]]
                ], columns=['Type', 'Count'])
                st.dataframe(cm_df, use_container_width=True)
            
            # Performance summary
            st.subheader("ğŸ“‹ LSTM Classification Summary")
            summary_data = {
                'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
                'Value': [
                    f"{metrics['test_accuracy']:.4f}",
                    f"{metrics['test_precision']:.4f}",
                    f"{metrics['test_recall']:.4f}",
                    f"{metrics['test_f1_score']:.4f}"
                ]
            }
            st.dataframe(pd.DataFrame(summary_data), use_container_width=True)
        
        # Feature Importance
        st.subheader("ğŸ” Feature Importance (Gradient-based)")
        if selected_model in self.feature_importance:
            importance_df = self.feature_importance[selected_model]
            if not importance_df.empty:
                # Show top 10 features in a bar chart
                top_features = importance_df.head(10)
                st.bar_chart(top_features.set_index('feature')['importance'])
                
                with st.expander("ğŸ“Š Full Feature Importance Table"):
                    st.dataframe(importance_df, use_container_width=True)
            else:
                st.warning("Feature importance not available for this model")
        else:
            st.warning("Feature importance not calculated for this model")

def create_comprehensive_lstm_models() -> SP500LSTMPredictor:
    """Create and train comprehensive LSTM models for both regression and classification"""
    predictor = SP500LSTMPredictor(sequence_length=20)  # Shorter sequence for faster training
    
    print("ğŸ“Š Training LSTM models...")
    
    try:
        # Classification (main use case for trading)
        print("ğŸ¯ Training classification model...")
        predictor.load_data(target_type='classification')
        predictor.train_model(model_name='optimized', epochs=25, batch_size=32)
        class_metrics = predictor.evaluate_model('optimized')
        print(f"âœ… Classification - Accuracy: {class_metrics['test_accuracy']:.4f}")
        
        # Regression
        print("ğŸ“ˆ Training regression model...")
        predictor.load_data(target_type='regression')
        predictor.train_model(model_name='optimized', epochs=25, batch_size=32)
        reg_metrics = predictor.evaluate_model('optimized')
        print(f"âœ… Regression - RMSE: {reg_metrics['test_rmse']:.6f}, RÂ²: {reg_metrics['test_r2']:.6f}")
        
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        return None
    
    return predictor

if __name__ == "__main__":
    if not PYTORCH_AVAILABLE:
        print("âŒ PyTorch not available. Please install it first.")
        print("   Run: pip install torch")
        sys.exit(1)
    
    print("ğŸš€ Training LSTM Models with PyTorch...")
    predictor = create_comprehensive_lstm_models()
    
    if predictor:
        print("âœ… LSTM models trained successfully!")
        
        # Show summary
        for model_name in predictor.models.keys():
            summary = predictor.get_model_summary(model_name)
            print(f"\nğŸ“Š Model: {model_name}")
            print(f"   Task: {summary['task_type']}")
            print(f"   Metrics: {summary['performance_metrics']}")
    else:
        print("âŒ Failed to train LSTM models")