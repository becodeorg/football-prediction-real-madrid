"""
LightGBM Predictive Model for S&P 500 Trading
Uses CSV data generated by the dataset module for training and prediction
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
import json

warnings.filterwarnings('ignore')

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    import lightgbm as lgb
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import cross_val_score, TimeSeriesSplit
    LIGHTGBM_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Some dependencies not available: {e}")
    print("   Install with: pip install lightgbm scikit-learn")
    LIGHTGBM_AVAILABLE = False
    
    # Create dummy lgb for type hints
    class DummyLGB:
        class LGBMModel:
            pass
        class LGBMRegressor:
            pass
        class LGBMClassifier:
            pass
    lgb = DummyLGB()

# Try to import streamlit for tabbed interface
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False


class SP500LightGBMPredictor:
    """
    LightGBM-based predictive model for S&P 500 price movements
    Loads data from CSV files and provides comprehensive ML pipeline
    """
    
    def __init__(self, csv_dir: str = None, model_save_dir: str = None):
        """
        Initialize the LightGBM predictor
        
        Args:
            csv_dir (str): Directory containing CSV files
            model_save_dir (str): Directory to save trained models
        """
        # Set directories
        if csv_dir is None:
            csv_dir = os.path.join(parent_dir, 'data', 'csv')
        
        if model_save_dir is None:
            model_save_dir = os.path.join(parent_dir, 'models', 'saved_models')
        
        self.csv_dir = csv_dir
        self.model_save_dir = model_save_dir
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        # Data storage
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        self.target_type = None
        
        # Models storage
        self.models = {}
        self.model_metrics = {}
        self.feature_importance = {}
        
        print(f"ğŸš€ SP500LightGBMPredictor initialized")
        print(f"   ğŸ“ CSV directory: {csv_dir}")
        print(f"   ğŸ’¾ Model save directory: {model_save_dir}")
    
    def get_latest_csv_files(self) -> Dict[str, str]:
        """
        Find the latest CSV files in the directory
        
        Returns:
            Dict[str, str]: Paths to latest CSV files
        """
        if not os.path.exists(self.csv_dir):
            raise FileNotFoundError(f"CSV directory not found: {self.csv_dir}")
        
        files = os.listdir(self.csv_dir)
        
        # Filter for our CSV files
        csv_types = {
            'raw': [f for f in files if f.startswith('GSPC_raw_')],
            'processed': [f for f in files if f.startswith('GSPC_processed_')],
            'train': [f for f in files if f.startswith('GSPC_train_')],
            'test': [f for f in files if f.startswith('GSPC_test_')]
        }
        
        # Get the most recent files
        latest_files = {}
        for file_type, file_list in csv_types.items():
            if file_list:
                latest_files[file_type] = os.path.join(self.csv_dir, sorted(file_list)[-1])
            else:
                latest_files[file_type] = None
        
        return latest_files
    
    def load_data(self, target_type: str = 'regression') -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        Load training and test data from CSV files
        
        Args:
            target_type (str): 'regression' for returns, 'classification' for direction
        
        Returns:
            Tuple: X_train, X_test, y_train, y_test
        """
        print(f"\nğŸ“Š Loading data for {target_type} task...")
        
        csv_files = self.get_latest_csv_files()
        
        if not csv_files['train'] or not csv_files['test']:
            raise FileNotFoundError("Training or test CSV files not found. Run create_csv_dataset.py first!")
        
        # Load data
        train_df = pd.read_csv(csv_files['train'], index_col=0, parse_dates=True)
        test_df = pd.read_csv(csv_files['test'], index_col=0, parse_dates=True)
        
        print(f"âœ… Loaded data:")
        print(f"   ğŸ“ˆ Training: {train_df.shape}")
        print(f"   ğŸ§ª Testing: {test_df.shape}")
        
        # For classification, we might need to load processed data with direction targets
        if target_type == 'classification':
            if not csv_files['processed']:
                raise FileNotFoundError("Processed CSV file needed for classification task")
            
            processed_df = pd.read_csv(csv_files['processed'], index_col=0, parse_dates=True)
            
            # Get features (same as in train/test CSV)
            feature_cols = [col for col in train_df.columns if col != 'target']
            
            # Align processed data with train/test split dates
            train_end_date = train_df.index[-1]
            test_start_date = test_df.index[0]
            
            # Create classification targets
            train_processed = processed_df.loc[:train_end_date]
            test_processed = processed_df.loc[test_start_date:]
            
            # Use binary direction target
            self.X_train = train_processed[feature_cols].dropna()
            self.y_train = train_processed.loc[self.X_train.index, 'target_direction_1d']
            
            self.X_test = test_processed[feature_cols].dropna()
            self.y_test = test_processed.loc[self.X_test.index, 'target_direction_1d']
            
            print(f"   ğŸ¯ Classification target distribution:")
            print(f"      Train: {self.y_train.value_counts().to_dict()}")
            print(f"      Test: {self.y_test.value_counts().to_dict()}")
            
        else:
            # Regression: use pre-prepared train/test data
            self.X_train = train_df.drop('target', axis=1)
            self.y_train = train_df['target']
            self.X_test = test_df.drop('target', axis=1)
            self.y_test = test_df['target']
            
            print(f"   ğŸ¯ Regression target stats:")
            print(f"      Train: mean={self.y_train.mean():.6f}, std={self.y_train.std():.6f}")
            print(f"      Test: mean={self.y_test.mean():.6f}, std={self.y_test.std():.6f}")
        
        self.feature_names = list(self.X_train.columns)
        self.target_type = target_type
        
        print(f"   ğŸ“‹ Features: {len(self.feature_names)}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def train_model(self, model_name: str = 'default', 
                   lgb_params: Dict = None, 
                   use_early_stopping: bool = True,
                   cv_folds: int = 3):
        """
        Train LightGBM model with comprehensive configuration
        
        Args:
            model_name (str): Name for the model
            lgb_params (Dict): LightGBM parameters
            use_early_stopping (bool): Whether to use early stopping
            cv_folds (int): Number of CV folds for validation
        
        Returns:
            lgb.LGBMModel: Trained model
        """
        if not LIGHTGBM_AVAILABLE:
            raise ImportError("LightGBM not available. Install with: pip install lightgbm scikit-learn")
        
        print(f"\nğŸ‹ï¸ Training LightGBM model: {model_name}")
        print("-" * 50)
        
        if self.X_train is None:
            raise ValueError("Data not loaded. Call load_data() first!")
        
        # Default parameters based on task type
        if lgb_params is None:
            if self.target_type == 'classification':
                lgb_params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'num_leaves': 31,
                    'learning_rate': 0.05,
                    'feature_fraction': 0.9,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5,
                    'verbose': -1,
                    'random_state': 42
                }
                model = lgb.LGBMClassifier(**lgb_params)
            else:
                lgb_params = {
                    'objective': 'regression',
                    'metric': 'rmse',
                    'boosting_type': 'gbdt',
                    'num_leaves': 31,
                    'learning_rate': 0.05,
                    'feature_fraction': 0.9,
                    'bagging_fraction': 0.8,
                    'bagging_freq': 5,
                    'verbose': -1,
                    'random_state': 42
                }
                model = lgb.LGBMRegressor(**lgb_params)
        else:
            if self.target_type == 'classification':
                model = lgb.LGBMClassifier(**lgb_params)
            else:
                model = lgb.LGBMRegressor(**lgb_params)
        
        # Cross-validation for model validation
        print(f"ğŸ“Š Performing {cv_folds}-fold cross-validation...")
        tscv = TimeSeriesSplit(n_splits=cv_folds)
        
        if self.target_type == 'classification':
            cv_scores = cross_val_score(model, self.X_train, self.y_train, 
                                      cv=tscv, scoring='accuracy')
            print(f"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        else:
            cv_scores = cross_val_score(model, self.X_train, self.y_train, 
                                      cv=tscv, scoring='r2')
            print(f"   CV RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # Train on full training set
        print("ğŸ”§ Training on full training set...")
        
        if use_early_stopping and len(self.X_train) > 100:
            # Use part of training data for validation
            val_size = min(200, len(self.X_train) // 5)
            X_val = self.X_train.iloc[-val_size:]
            y_val = self.y_train.iloc[-val_size:]
            X_train_fit = self.X_train.iloc[:-val_size]
            y_train_fit = self.y_train.iloc[:-val_size]
            
            model.fit(
                X_train_fit, y_train_fit,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
            )
        else:
            model.fit(self.X_train, self.y_train)
        
        # Store model and get feature importance
        self.models[model_name] = model
        self.feature_importance[model_name] = pd.DataFrame({
            'feature': self.feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"âœ… Model '{model_name}' trained successfully!")
        print(f"   ğŸŒŸ Top 5 features: {list(self.feature_importance[model_name]['feature'].head())}")
        
        return model
    
    def evaluate_model(self, model_name: str = 'default') -> Dict:
        """
        Comprehensive model evaluation
        
        Args:
            model_name (str): Name of the model to evaluate
        
        Returns:
            Dict: Evaluation metrics
        """
        print(f"\nğŸ“ˆ Evaluating model: {model_name}")
        print("-" * 40)
        
        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not found. Train it first!")
        
        model = self.models[model_name]
        
        # Make predictions
        train_pred = model.predict(self.X_train)
        test_pred = model.predict(self.X_test)
        
        # Calculate metrics based on task type
        if self.target_type == 'classification':
            # Classification metrics
            train_accuracy = accuracy_score(self.y_train, train_pred)
            test_accuracy = accuracy_score(self.y_test, test_pred)
            
            # Get probabilities for more detailed analysis
            if hasattr(model, 'predict_proba'):
                train_proba = model.predict_proba(self.X_train)[:, 1]
                test_proba = model.predict_proba(self.X_test)[:, 1]
            else:
                train_proba = train_pred
                test_proba = test_pred
            
            metrics = {
                'task_type': 'classification',
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'test_precision': precision_score(self.y_test, test_pred, average='weighted'),
                'test_recall': recall_score(self.y_test, test_pred, average='weighted'),
                'test_f1': f1_score(self.y_test, test_pred, average='weighted'),
                'predictions': {
                    'train': train_pred,
                    'test': test_pred,
                    'train_proba': train_proba,
                    'test_proba': test_proba
                }
            }
            
            print(f"ğŸ“Š Classification Results:")
            print(f"   ğŸ‹ï¸ Training Accuracy: {train_accuracy:.4f}")
            print(f"   ğŸ§ª Test Accuracy: {test_accuracy:.4f}")
            print(f"   ğŸ¯ Test Precision: {metrics['test_precision']:.4f}")
            print(f"   ğŸ” Test Recall: {metrics['test_recall']:.4f}")
            print(f"   âš–ï¸ Test F1-Score: {metrics['test_f1']:.4f}")
            
        else:
            # Regression metrics
            train_rmse = np.sqrt(mean_squared_error(self.y_train, train_pred))
            test_rmse = np.sqrt(mean_squared_error(self.y_test, test_pred))
            train_mae = mean_absolute_error(self.y_train, train_pred)
            test_mae = mean_absolute_error(self.y_test, test_pred)
            train_r2 = r2_score(self.y_train, train_pred)
            test_r2 = r2_score(self.y_test, test_pred)
            
            metrics = {
                'task_type': 'regression',
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'predictions': {
                    'train': train_pred,
                    'test': test_pred
                }
            }
            
            print(f"ğŸ“Š Regression Results:")
            print(f"   ğŸ‹ï¸ Training RMSE: {train_rmse:.6f}")
            print(f"   ğŸ§ª Test RMSE: {test_rmse:.6f}")
            print(f"   ğŸ‹ï¸ Training MAE: {train_mae:.6f}")
            print(f"   ğŸ§ª Test MAE: {test_mae:.6f}")
            print(f"   ğŸ‹ï¸ Training RÂ²: {train_r2:.6f}")
            print(f"   ğŸ§ª Test RÂ²: {test_r2:.6f}")
        
        # Store metrics
        self.model_metrics[model_name] = metrics
        
        return metrics
    
    def get_model_summary(self, model_name: str = 'default') -> Dict:
        """
        Get comprehensive model performance summary
        
        Args:
            model_name (str): Name of the model to summarize
        
        Returns:
            Dict: Complete model performance summary
        """
        if model_name not in self.model_metrics:
            raise ValueError(f"Model '{model_name}' not evaluated yet!")
        
        metrics = self.model_metrics[model_name]
        model = self.models[model_name]
        
        # Get feature importance
        importance = self.feature_importance[model_name].head(10)
        
        summary = {
            'model_name': model_name,
            'task_type': metrics['task_type'],
            'data_info': {
                'train_samples': len(self.X_train),
                'test_samples': len(self.X_test),
                'features': len(self.feature_names),
                'train_period': f"{self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}",
                'test_period': f"{self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}"
            },
            'performance_metrics': {},
            'top_features': importance.to_dict('records'),
            'model_params': model.get_params()
        }
        
        if metrics['task_type'] == 'classification':
            # Calculate additional classification metrics
            test_pred = metrics['predictions']['test']
            train_pred = metrics['predictions']['train']
            
            # Confusion matrix analysis
            cm = confusion_matrix(self.y_test, test_pred)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
            
            # Calculate confidence intervals (rough approximation)
            n_test = len(self.y_test)
            accuracy_ci = 1.96 * np.sqrt((metrics['test_accuracy'] * (1 - metrics['test_accuracy'])) / n_test)
            
            summary['performance_metrics'] = {
                'train_accuracy': metrics['train_accuracy'],
                'test_accuracy': metrics['test_accuracy'],
                'accuracy_confidence_interval': f"Â±{accuracy_ci:.4f}",
                'test_precision': metrics['test_precision'],
                'test_recall': metrics['test_recall'],
                'test_f1_score': metrics['test_f1'],
                'confusion_matrix': {
                    'true_negative': int(tn),
                    'false_positive': int(fp),
                    'false_negative': int(fn),
                    'true_positive': int(tp)
                },
                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
                'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,
                'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,
                'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0
            }
            
        else:
            # Regression metrics with confidence analysis
            test_pred = metrics['predictions']['test']
            train_pred = metrics['predictions']['train']
            
            # Additional regression metrics
            test_residuals = self.y_test - test_pred
            train_residuals = self.y_train - train_pred
            
            # Mean Absolute Percentage Error
            mape_test = np.mean(np.abs((self.y_test - test_pred) / np.where(self.y_test != 0, self.y_test, 1))) * 100
            mape_train = np.mean(np.abs((self.y_train - train_pred) / np.where(self.y_train != 0, self.y_train, 1))) * 100
            
            # Directional Accuracy (for returns prediction)
            direction_accuracy_test = np.mean(np.sign(self.y_test) == np.sign(test_pred))
            direction_accuracy_train = np.mean(np.sign(self.y_train) == np.sign(train_pred))
            
            summary['performance_metrics'] = {
                'train_rmse': metrics['train_rmse'],
                'test_rmse': metrics['test_rmse'],
                'train_mae': metrics['train_mae'],
                'test_mae': metrics['test_mae'],
                'train_r2': metrics['train_r2'],
                'test_r2': metrics['test_r2'],
                'train_mape': mape_train,
                'test_mape': mape_test,
                'train_direction_accuracy': direction_accuracy_train,
                'test_direction_accuracy': direction_accuracy_test,
                'residual_stats': {
                    'test_residual_mean': float(test_residuals.mean()),
                    'test_residual_std': float(test_residuals.std()),
                    'test_residual_skew': float(test_residuals.skew()),
                    'test_residual_kurtosis': float(test_residuals.kurtosis())
                },
                'prediction_bounds': {
                    'test_pred_min': float(test_pred.min()),
                    'test_pred_max': float(test_pred.max()),
                    'test_pred_mean': float(test_pred.mean()),
                    'test_pred_std': float(test_pred.std())
                }
            }
        
        return summary
    
    def print_performance_report(self, model_name: str = 'default') -> None:
        """
        Print a comprehensive performance report
        
        Args:
            model_name (str): Name of the model to report on
        """
        summary = self.get_model_summary(model_name)
        
        print(f"\nğŸ“Š PERFORMANCE REPORT: {summary['model_name']}")
        print("=" * 60)
        
        # Data Info
        print(f"ğŸ“ˆ DATA INFORMATION:")
        data_info = summary['data_info']
        print(f"   Training Samples: {data_info['train_samples']:,}")
        print(f"   Test Samples: {data_info['test_samples']:,}")
        print(f"   Features: {data_info['features']}")
        print(f"   Train Period: {data_info['train_period']}")
        print(f"   Test Period: {data_info['test_period']}")
        
        # Performance Metrics
        print(f"\nğŸ¯ PERFORMANCE METRICS:")
        metrics = summary['performance_metrics']
        
        if summary['task_type'] == 'classification':
            print(f"   âœ… Test Accuracy: {metrics['test_accuracy']:.4f} {metrics['accuracy_confidence_interval']}")
            print(f"   ğŸ¯ Test Precision: {metrics['test_precision']:.4f}")
            print(f"   ğŸ” Test Recall: {metrics['test_recall']:.4f}")
            print(f"   âš–ï¸ Test F1-Score: {metrics['test_f1_score']:.4f}")
            print(f"   ğŸª Specificity: {metrics['specificity']:.4f}")
            print(f"   ğŸ“¡ Sensitivity: {metrics['sensitivity']:.4f}")
            print(f"   âŒ False Positive Rate: {metrics['false_positive_rate']:.4f}")
            print(f"   âŒ False Negative Rate: {metrics['false_negative_rate']:.4f}")
            
            print(f"\nğŸ“‹ CONFUSION MATRIX:")
            cm = metrics['confusion_matrix']
            print(f"   True Negative: {cm['true_negative']}")
            print(f"   False Positive: {cm['false_positive']}")
            print(f"   False Negative: {cm['false_negative']}")
            print(f"   True Positive: {cm['true_positive']}")
            
        else:
            print(f"   ğŸ“Š Test RMSE: {metrics['test_rmse']:.6f}")
            print(f"   ğŸ“Š Test MAE: {metrics['test_mae']:.6f}")
            print(f"   ğŸ“Š Test RÂ²: {metrics['test_r2']:.6f}")
            print(f"   ğŸ“Š Test MAPE: {metrics['test_mape']:.2f}%")
            print(f"   ğŸ¯ Direction Accuracy: {metrics['test_direction_accuracy']:.4f}")
            
            print(f"\nğŸ“ˆ RESIDUAL ANALYSIS:")
            residuals = metrics['residual_stats']
            print(f"   Mean: {residuals['test_residual_mean']:.6f}")
            print(f"   Std: {residuals['test_residual_std']:.6f}")
            print(f"   Skewness: {residuals['test_residual_skew']:.4f}")
            print(f"   Kurtosis: {residuals['test_residual_kurtosis']:.4f}")
            
            print(f"\nğŸ” PREDICTION ANALYSIS:")
            pred_bounds = metrics['prediction_bounds']
            print(f"   Min Prediction: {pred_bounds['test_pred_min']:.6f}")
            print(f"   Max Prediction: {pred_bounds['test_pred_max']:.6f}")
            print(f"   Mean Prediction: {pred_bounds['test_pred_mean']:.6f}")
            print(f"   Std Prediction: {pred_bounds['test_pred_std']:.6f}")
        
        # Feature Importance
        print(f"\nâ­ TOP 10 FEATURES:")
        for i, feature in enumerate(summary['top_features'], 1):
            print(f"   {i:2d}. {feature['feature']:<20} ({feature['importance']:.4f})")
        
        print(f"\n" + "=" * 60)
    
    def create_performance_dashboard(self) -> None:
        """
        Create an interactive dashboard using Streamlit for model performance comparison
        """
        if not STREAMLIT_AVAILABLE:
            print("âš ï¸  Streamlit not available. Install with: pip install streamlit")
            self.create_console_dashboard()
            return
        
        st.set_page_config(
            page_title="S&P 500 LightGBM Performance Dashboard",
            page_icon="ğŸ“Š",
            layout="wide"
        )
        
        st.title("ğŸš€ S&P 500 LightGBM Performance Dashboard")
        st.markdown("---")
        
        if not self.models:
            st.error("No models found! Please train models first.")
            return
        
        # Sidebar for model selection
        st.sidebar.header("ğŸ›ï¸ Dashboard Controls")
        
        # Model filtering
        model_types = ['All', 'Regression', 'Classification']
        selected_type = st.sidebar.selectbox("Filter by Model Type:", model_types)
        
        # Get filtered models
        filtered_models = self._filter_models_by_type(selected_type)
        
        if not filtered_models:
            st.error(f"No {selected_type.lower()} models found!")
            return
        
        # Main dashboard tabs
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š Overview", 
            "ğŸ¯ Performance Metrics", 
            "â­ Feature Importance", 
            "ğŸ”® Future Predictions",
            "ğŸ“ˆ Model Comparison"
        ])
        
        with tab1:
            self._render_overview_tab(filtered_models)
        
        with tab2:
            self._render_performance_tab(filtered_models)
        
        with tab3:
            self._render_feature_importance_tab(filtered_models)
        
        with tab4:
            self._render_predictions_tab(filtered_models)
        
        with tab5:
            self._render_comparison_tab(filtered_models)
    
    def create_console_dashboard(self) -> None:
        """
        Create a console-based tabbed dashboard for model performance comparison
        """
        print("\n" + "="*80)
        print("ğŸš€ S&P 500 LIGHTGBM PERFORMANCE DASHBOARD")
        print("="*80)
        
        if not self.models:
            print("âŒ No models found! Please train models first.")
            return
        
        while True:
            print(f"\nğŸ“‹ AVAILABLE OPTIONS:")
            print("   1ï¸âƒ£  Overview & Data Info")
            print("   2ï¸âƒ£  Performance Metrics")
            print("   3ï¸âƒ£  Feature Importance Analysis")
            print("   4ï¸âƒ£  Future Predictions")
            print("   5ï¸âƒ£  Model Comparison")
            print("   6ï¸âƒ£  Export Results")
            print("   0ï¸âƒ£  Exit Dashboard")
            
            try:
                choice = input("\nğŸ¯ Select option (0-6): ").strip()
                
                if choice == "0":
                    print("ğŸ‘‹ Exiting dashboard...")
                    break
                elif choice == "1":
                    self._console_overview()
                elif choice == "2":
                    self._console_performance_metrics()
                elif choice == "3":
                    self._console_feature_importance()
                elif choice == "4":
                    self._console_future_predictions()
                elif choice == "5":
                    self._console_model_comparison()
                elif choice == "6":
                    self._console_export_results()
                else:
                    print("âŒ Invalid choice. Please select 0-6.")
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Dashboard interrupted. Exiting...")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")
    
    def _filter_models_by_type(self, model_type: str) -> List[str]:
        """Filter models by type"""
        if model_type == 'All':
            return list(self.models.keys())
        elif model_type == 'Regression':
            return [name for name in self.models.keys() if 'regression' in name.lower()]
        elif model_type == 'Classification':
            return [name for name in self.models.keys() if 'classification' in name.lower()]
        return []
    
    def _render_overview_tab(self, models: List[str]) -> None:
        """Render the overview tab in Streamlit"""
        st.header("ğŸ“ˆ Dataset & Models Overview")
        
        # Dataset info
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Models", len(models))
            st.metric("Training Samples", len(self.X_train) if self.X_train is not None else 0)
        
        with col2:
            st.metric("Test Samples", len(self.X_test) if self.X_test is not None else 0)
            st.metric("Features", len(self.feature_names) if self.feature_names else 0)
        
        with col3:
            if self.X_train is not None:
                train_period = f"{self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}"
                test_period = f"{self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}"
                st.write(f"**Train Period:** {train_period}")
                st.write(f"**Test Period:** {test_period}")
        
        # Models summary
        st.subheader("ğŸ¤– Trained Models")
        models_data = []
        for model_name in models:
            if model_name in self.model_metrics:
                summary = self.get_model_summary(model_name)
                models_data.append({
                    'Model': model_name,
                    'Type': summary['task_type'].title(),
                    'Features': summary['data_info']['features'],
                    'Status': 'âœ… Trained & Evaluated'
                })
            else:
                models_data.append({
                    'Model': model_name,
                    'Type': 'Unknown',
                    'Features': 'N/A',
                    'Status': 'âš ï¸ Trained Only'
                })
        
        st.dataframe(pd.DataFrame(models_data), use_container_width=True)
    
    def _render_performance_tab(self, models: List[str]) -> None:
        """Render the performance metrics tab in Streamlit"""
        st.header("ğŸ¯ Performance Metrics")
        
        selected_model = st.selectbox("Select Model:", models)
        
        if selected_model not in self.model_metrics:
            st.error(f"Model '{selected_model}' not evaluated yet!")
            return
        
        summary = self.get_model_summary(selected_model)
        metrics = summary['performance_metrics']
        
        # Performance metrics display
        if summary['task_type'] == 'regression':
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Test RMSE", f"{metrics['test_rmse']:.6f}")
                st.metric("Test MAE", f"{metrics['test_mae']:.6f}")
            
            with col2:
                st.metric("Test RÂ²", f"{metrics['test_r2']:.6f}")
                st.metric("Test MAPE", f"{metrics['test_mape']:.2f}%")
            
            with col3:
                st.metric("Direction Accuracy", f"{metrics['test_direction_accuracy']:.4f}")
                
            with col4:
                st.write("**Residual Stats:**")
                st.write(f"Mean: {metrics['residual_stats']['test_residual_mean']:.6f}")
                st.write(f"Std: {metrics['residual_stats']['test_residual_std']:.6f}")
                st.write(f"Skewness: {metrics['residual_stats']['test_residual_skew']:.4f}")
        
        else:  # Classification
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test Accuracy", f"{metrics['test_accuracy']:.4f}")
                st.metric("Test Precision", f"{metrics['test_precision']:.4f}")
            
            with col2:
                st.metric("Test Recall", f"{metrics['test_recall']:.4f}")
                st.metric("Test F1-Score", f"{metrics['test_f1_score']:.4f}")
            
            with col3:
                st.metric("Specificity", f"{metrics['specificity']:.4f}")
                st.metric("Sensitivity", f"{metrics['sensitivity']:.4f}")
            
            # Confusion Matrix
            st.subheader("ğŸ“‹ Confusion Matrix")
            cm = metrics['confusion_matrix']
            cm_df = pd.DataFrame([
                ['True Negative', cm['true_negative']],
                ['False Positive', cm['false_positive']],
                ['False Negative', cm['false_negative']],
                ['True Positive', cm['true_positive']]
            ], columns=['Metric', 'Value'])
            
            st.dataframe(cm_df, use_container_width=True)
    
    def _render_feature_importance_tab(self, models: List[str]) -> None:
        """Render the feature importance tab in Streamlit"""
        st.header("â­ Feature Importance Analysis")
        
        selected_model = st.selectbox("Select Model:", models, key="feat_model")
        
        if selected_model not in self.feature_importance:
            st.error(f"Feature importance not available for '{selected_model}'!")
            return
        
        importance_df = self.feature_importance[selected_model].head(15)
        
        # Bar chart
        st.subheader(f"ğŸ” Top 15 Features - {selected_model}")
        st.bar_chart(importance_df.set_index('feature')['importance'])
        
        # Table
        st.subheader("ğŸ“Š Feature Importance Table")
        st.dataframe(importance_df, use_container_width=True)
    
    def _render_predictions_tab(self, models: List[str]) -> None:
        """Render the future predictions tab in Streamlit"""
        st.header("ğŸ”® Future Predictions")
        
        selected_model = st.selectbox("Select Model:", models, key="pred_model")
        days_ahead = st.slider("Prediction Days", 1, 10, 5)
        
        if st.button("Generate Predictions"):
            try:
                predictions = self.predict_future(selected_model, days_ahead=days_ahead)
                
                st.subheader(f"ğŸ“ˆ {days_ahead}-Day Predictions - {selected_model}")
                
                pred_data = []
                for i, (date, pred) in enumerate(zip(predictions['dates'], predictions['predictions'])):
                    if predictions['prediction_type'] == 'classification':
                        direction = "ğŸ“ˆ UP" if pred['prediction'] == 1 else "ğŸ“‰ DOWN"
                        pred_data.append({
                            'Date': date.strftime('%Y-%m-%d'),
                            'Day': i+1,
                            'Prediction': direction,
                            'Probability': f"{pred['probability']:.3f}"
                        })
                    else:
                        pred_data.append({
                            'Date': date.strftime('%Y-%m-%d'),
                            'Day': i+1,
                            'Return': f"{pred:+.6f}",
                            'Direction': "ğŸ“ˆ" if pred > 0 else "ğŸ“‰"
                        })
                
                st.dataframe(pd.DataFrame(pred_data), use_container_width=True)
                
            except Exception as e:
                st.error(f"Prediction failed: {e}")
    
    def _render_comparison_tab(self, models: List[str]) -> None:
        """Render the model comparison tab in Streamlit"""
        st.header("ğŸ“ˆ Model Comparison")
        
        if len(models) < 2:
            st.warning("Need at least 2 models for comparison!")
            return
        
        # Separate regression and classification models
        reg_models = [m for m in models if 'regression' in m.lower() and m in self.model_metrics]
        clf_models = [m for m in models if 'classification' in m.lower() and m in self.model_metrics]
        
        if reg_models:
            st.subheader("ğŸ”¢ Regression Models Comparison")
            reg_comparison = []
            for model_name in reg_models:
                summary = self.get_model_summary(model_name)
                metrics = summary['performance_metrics']
                reg_comparison.append({
                    'Model': model_name,
                    'RMSE': f"{metrics['test_rmse']:.6f}",
                    'MAE': f"{metrics['test_mae']:.6f}",
                    'RÂ²': f"{metrics['test_r2']:.6f}",
                    'Direction Accuracy': f"{metrics['test_direction_accuracy']:.4f}",
                    'MAPE': f"{metrics['test_mape']:.2f}%"
                })
            
            reg_df = pd.DataFrame(reg_comparison)
            st.dataframe(reg_df, use_container_width=True)
            
            # Best model highlight
            best_r2_model = max(reg_models, key=lambda x: self.get_model_summary(x)['performance_metrics']['test_r2'])
            st.success(f"ğŸ† Best RÂ² Score: **{best_r2_model}**")
        
        if clf_models:
            st.subheader("ğŸ¯ Classification Models Comparison")
            clf_comparison = []
            for model_name in clf_models:
                summary = self.get_model_summary(model_name)
                metrics = summary['performance_metrics']
                clf_comparison.append({
                    'Model': model_name,
                    'Accuracy': f"{metrics['test_accuracy']:.4f}",
                    'Precision': f"{metrics['test_precision']:.4f}",
                    'Recall': f"{metrics['test_recall']:.4f}",
                    'F1-Score': f"{metrics['test_f1_score']:.4f}"
                })
            
            clf_df = pd.DataFrame(clf_comparison)
            st.dataframe(clf_df, use_container_width=True)
            
            # Best model highlight
            best_f1_model = max(clf_models, key=lambda x: self.get_model_summary(x)['performance_metrics']['test_f1_score'])
            st.success(f"ğŸ† Best F1-Score: **{best_f1_model}**")
    
    def _console_overview(self) -> None:
        """Console overview display"""
        print("\n" + "="*60)
        print("ğŸ“ˆ DATASET & MODELS OVERVIEW")
        print("="*60)
        
        print(f"ğŸ¤– Total Models: {len(self.models)}")
        if self.X_train is not None:
            print(f"ğŸ“Š Training Samples: {len(self.X_train):,}")
            print(f"ğŸ§ª Test Samples: {len(self.X_test):,}")
            print(f"ğŸ“‹ Features: {len(self.feature_names)}")
            print(f"ğŸ“… Train Period: {self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}")
            print(f"ğŸ“… Test Period: {self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}")
        
        print(f"\nğŸ¤– MODEL STATUS:")
        for model_name in self.models.keys():
            status = "âœ… Trained & Evaluated" if model_name in self.model_metrics else "âš ï¸ Trained Only"
            model_type = "Regression" if "regression" in model_name else "Classification"
            print(f"   {model_name} ({model_type}): {status}")
    
    def _console_performance_metrics(self) -> None:
        """Console performance metrics display"""
        print("\n" + "="*60)
        print("ğŸ¯ PERFORMANCE METRICS")
        print("="*60)
        
        evaluated_models = [name for name in self.models.keys() if name in self.model_metrics]
        
        if not evaluated_models:
            print("âŒ No evaluated models found!")
            return
        
        print("ğŸ“Š Available Models:")
        for i, model_name in enumerate(evaluated_models, 1):
            print(f"   {i}. {model_name}")
        
        try:
            choice = int(input("\nğŸ¯ Select model number: ")) - 1
            if 0 <= choice < len(evaluated_models):
                selected_model = evaluated_models[choice]
                self.print_performance_report(selected_model)
            else:
                print("âŒ Invalid selection!")
        except ValueError:
            print("âŒ Please enter a valid number!")
    
    def _console_feature_importance(self) -> None:
        """Console feature importance display"""
        print("\n" + "="*60)
        print("â­ FEATURE IMPORTANCE ANALYSIS")
        print("="*60)
        
        models_with_importance = [name for name in self.models.keys() if name in self.feature_importance]
        
        if not models_with_importance:
            print("âŒ No models with feature importance found!")
            return
        
        print("ğŸ“Š Available Models:")
        for i, model_name in enumerate(models_with_importance, 1):
            print(f"   {i}. {model_name}")
        
        try:
            choice = int(input("\nğŸ¯ Select model number: ")) - 1
            if 0 <= choice < len(models_with_importance):
                selected_model = models_with_importance[choice]
                print(f"\nâ­ TOP 15 FEATURES - {selected_model}")
                print("-" * 50)
                
                importance_df = self.feature_importance[selected_model].head(15)
                for i, row in importance_df.iterrows():
                    print(f"   {i+1:2d}. {row['feature']:<25} ({row['importance']:.4f})")
            else:
                print("âŒ Invalid selection!")
        except ValueError:
            print("âŒ Please enter a valid number!")
    
    def _console_future_predictions(self) -> None:
        """Console future predictions display"""
        print("\n" + "="*60)
        print("ğŸ”® FUTURE PREDICTIONS")
        print("="*60)
        
        if not self.models:
            print("âŒ No models found!")
            return
        
        print("ğŸ“Š Available Models:")
        model_names = list(self.models.keys())
        for i, model_name in enumerate(model_names, 1):
            print(f"   {i}. {model_name}")
        
        try:
            choice = int(input("\nğŸ¯ Select model number: ")) - 1
            if 0 <= choice < len(model_names):
                selected_model = model_names[choice]
                
                days = int(input("ğŸ“… Enter prediction days (1-10): "))
                if 1 <= days <= 10:
                    predictions = self.predict_future(selected_model, days_ahead=days)
                    
                    print(f"\nğŸ”® {days}-DAY PREDICTIONS - {selected_model}")
                    print("-" * 50)
                    
                    for i, (date, pred) in enumerate(zip(predictions['dates'], predictions['predictions'])):
                        if predictions['prediction_type'] == 'classification':
                            direction = "ğŸ“ˆ UP" if pred['prediction'] == 1 else "ğŸ“‰ DOWN"
                            print(f"   {date.strftime('%Y-%m-%d')}: {direction} (prob: {pred['probability']:.3f})")
                        else:
                            direction = "ğŸ“ˆ" if pred > 0 else "ğŸ“‰"
                            print(f"   {date.strftime('%Y-%m-%d')}: {pred:+.6f} return {direction}")
                else:
                    print("âŒ Please enter days between 1-10!")
            else:
                print("âŒ Invalid selection!")
        except ValueError:
            print("âŒ Please enter valid numbers!")
        except Exception as e:
            print(f"âŒ Prediction failed: {e}")
    
    def _console_model_comparison(self) -> None:
        """Console model comparison display"""
        print("\n" + "="*80)
        print("ğŸ“ˆ MODEL COMPARISON")
        print("="*80)
        
        evaluated_models = [name for name in self.models.keys() if name in self.model_metrics]
        
        if len(evaluated_models) < 2:
            print("âŒ Need at least 2 evaluated models for comparison!")
            return
        
        # Separate model types
        reg_models = [m for m in evaluated_models if 'regression' in m.lower()]
        clf_models = [m for m in evaluated_models if 'classification' in m.lower()]
        
        if reg_models:
            print("\nğŸ”¢ REGRESSION MODELS COMPARISON")
            print("-" * 70)
            print(f"{'Model':<25} {'RMSE':<12} {'RÂ²':<12} {'Direction Acc':<15}")
            print("-" * 70)
            
            best_r2 = -float('inf')
            best_model = ""
            
            for model_name in reg_models:
                summary = self.get_model_summary(model_name)
                metrics = summary['performance_metrics']
                
                print(f"{model_name:<25} {metrics['test_rmse']:<12.6f} {metrics['test_r2']:<12.6f} {metrics['test_direction_accuracy']:<15.4f}")
                
                if metrics['test_r2'] > best_r2:
                    best_r2 = metrics['test_r2']
                    best_model = model_name
            
            print(f"\nğŸ† Best RÂ² Score: {best_model} ({best_r2:.6f})")
        
        if clf_models:
            print("\nğŸ¯ CLASSIFICATION MODELS COMPARISON")
            print("-" * 80)
            print(f"{'Model':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
            print("-" * 80)
            
            best_f1 = -1
            best_model = ""
            
            for model_name in clf_models:
                summary = self.get_model_summary(model_name)
                metrics = summary['performance_metrics']
                
                print(f"{model_name:<25} {metrics['test_accuracy']:<12.4f} {metrics['test_precision']:<12.4f} {metrics['test_recall']:<12.4f} {metrics['test_f1_score']:<12.4f}")
                
                if metrics['test_f1_score'] > best_f1:
                    best_f1 = metrics['test_f1_score']
                    best_model = model_name
            
            print(f"\nğŸ† Best F1-Score: {best_model} ({best_f1:.4f})")
    
    def _console_export_results(self) -> None:
        """Console export results"""
        print("\n" + "="*60)
        print("ğŸ“¤ EXPORT RESULTS")
        print("="*60)
        
        if not self.model_metrics:
            print("âŒ No evaluation results to export!")
            return
        
        try:
            # Create export directory
            export_dir = os.path.join(self.model_save_dir, 'exports')
            os.makedirs(export_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Export model summaries
            all_summaries = {}
            for model_name in self.model_metrics.keys():
                summary = self.get_model_summary(model_name)
                all_summaries[model_name] = summary
            
            # Save as JSON
            json_path = os.path.join(export_dir, f'model_results_{timestamp}.json')
            with open(json_path, 'w') as f:
                json.dump(all_summaries, f, indent=2, default=str)
            
            print(f"âœ… Results exported to: {json_path}")
            
            # Create summary CSV for easy viewing
            csv_data = []
            for model_name, summary in all_summaries.items():
                row = {
                    'Model': model_name,
                    'Type': summary['task_type'],
                    'Features': summary['data_info']['features']
                }
                
                metrics = summary['performance_metrics']
                if summary['task_type'] == 'regression':
                    row.update({
                        'RMSE': metrics['test_rmse'],
                        'RÂ²': metrics['test_r2'],
                        'MAE': metrics['test_mae'],
                        'Direction_Accuracy': metrics['test_direction_accuracy']
                    })
                else:
                    row.update({
                        'Accuracy': metrics['test_accuracy'],
                        'Precision': metrics['test_precision'],
                        'Recall': metrics['test_recall'],
                        'F1_Score': metrics['test_f1_score']
                    })
                
                csv_data.append(row)
            
            csv_path = os.path.join(export_dir, f'model_summary_{timestamp}.csv')
            pd.DataFrame(csv_data).to_csv(csv_path, index=False)
            
            print(f"âœ… Summary exported to: {csv_path}")
            
        except Exception as e:
            print(f"âŒ Export failed: {e}")
    
    def save_model(self, model_name: str = 'default') -> str:
        """
        Save trained model and metadata
        
        Args:
            model_name (str): Name of the model to save
        
        Returns:
            str: Path where model was saved
        """
        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not found!")
        
        # Create model directory
        model_dir = os.path.join(self.model_save_dir, f'lightgbm_{model_name}')
        os.makedirs(model_dir, exist_ok=True)
        
        # Save model
        model_path = os.path.join(model_dir, 'model.joblib')
        joblib.dump(self.models[model_name], model_path)
        
        # Save metadata
        metadata = {
            'model_name': model_name,
            'target_type': self.target_type,
            'feature_names': self.feature_names,
            'training_date': datetime.now().isoformat(),
            'data_shape': {
                'train': self.X_train.shape,
                'test': self.X_test.shape
            }
        }
        
        if model_name in self.model_metrics:
            metrics_to_save = self.model_metrics[model_name].copy()
            # Remove prediction arrays (too large for JSON)
            if 'predictions' in metrics_to_save:
                del metrics_to_save['predictions']
            metadata['metrics'] = metrics_to_save
        
        metadata_path = os.path.join(model_dir, 'metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # Save feature importance
        if model_name in self.feature_importance:
            importance_path = os.path.join(model_dir, 'feature_importance.csv')
            self.feature_importance[model_name].to_csv(importance_path, index=False)
        
        print(f"ğŸ’¾ Model '{model_name}' saved to: {model_dir}")
        return model_dir
    
    def load_model(self, model_path: str, model_name: str = None) -> str:
        """
        Load a saved model
        
        Args:
            model_path (str): Path to the saved model directory
            model_name (str): Name to assign to the loaded model
        
        Returns:
            str: Name of the loaded model
        """
        if model_name is None:
            model_name = os.path.basename(model_path).replace('lightgbm_', '')
        
        # Load model
        model_file = os.path.join(model_path, 'model.joblib')
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"Model file not found: {model_file}")
        
        self.models[model_name] = joblib.load(model_file)
        
        # Load metadata
        metadata_file = os.path.join(model_path, 'metadata.json')
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            self.feature_names = metadata.get('feature_names', [])
            self.target_type = metadata.get('target_type', 'regression')
            
            if 'metrics' in metadata:
                self.model_metrics[model_name] = metadata['metrics']
        
        # Load feature importance
        importance_file = os.path.join(model_path, 'feature_importance.csv')
        if os.path.exists(importance_file):
            self.feature_importance[model_name] = pd.read_csv(importance_file)
        
        print(f"ğŸ“ Model '{model_name}' loaded from: {model_path}")
        return model_name
    
    def predict_future(self, model_name: str = 'default', 
                      days_ahead: int = 5) -> Dict:
        """
        Make future predictions using the latest available data
        
        Args:
            model_name (str): Name of the model to use
            days_ahead (int): Number of days to predict ahead
        
        Returns:
            Dict: Prediction results
        """
        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not found!")
        
        print(f"\nğŸ”® Making {days_ahead}-day ahead predictions with {model_name}...")
        
        # Use the last row of test data as the most recent
        latest_features = self.X_test.iloc[-1:].copy()
        model = self.models[model_name]
        
        predictions = []
        dates = []
        
        # Start from the day after the last test date
        last_date = self.X_test.index[-1]
        
        for i in range(days_ahead):
            # Make prediction
            if self.target_type == 'classification':
                pred = model.predict(latest_features)[0]
                prob = model.predict_proba(latest_features)[0, 1] if hasattr(model, 'predict_proba') else pred
                predictions.append({'prediction': pred, 'probability': prob})
            else:
                pred = model.predict(latest_features)[0]
                predictions.append(pred)
            
            # Calculate next date (skip weekends for trading days)
            next_date = last_date + timedelta(days=i+1)
            while next_date.weekday() >= 5:  # Skip Saturday(5) and Sunday(6)
                next_date += timedelta(days=1)
            dates.append(next_date)
        
        result = {
            'model_name': model_name,
            'prediction_type': self.target_type,
            'start_date': dates[0],
            'end_date': dates[-1],
            'predictions': predictions,
            'dates': dates,
            'latest_features_date': latest_features.index[0]
        }
        
        print(f"âœ… Generated {days_ahead} predictions:")
        for i, (date, pred) in enumerate(zip(dates, predictions)):
            if self.target_type == 'classification':
                direction = "ğŸ“ˆ UP" if pred['prediction'] == 1 else "ğŸ“‰ DOWN"
                print(f"   {date.strftime('%Y-%m-%d')}: {direction} (prob: {pred['probability']:.3f})")
            else:
                print(f"   {date.strftime('%Y-%m-%d')}: {pred:+.6f} return")
        
        return result


def create_comprehensive_lgb_models() -> SP500LightGBMPredictor:
    """
    Create and train multiple LightGBM models for comprehensive analysis
    
    Returns:
        SP500LightGBMPredictor: Configured predictor with trained models
    """
    print("ğŸš€ Creating Comprehensive LightGBM Models")
    print("=" * 60)
    
    # Initialize predictor
    predictor = SP500LightGBMPredictor()
    
    # Train regression model
    print("\n1ï¸âƒ£ REGRESSION MODEL (Price Returns)")
    print("=" * 40)
    predictor.load_data(target_type='regression')
    predictor.train_model(model_name='regression_default')
    reg_metrics = predictor.evaluate_model('regression_default')
    predictor.print_performance_report('regression_default')
    
    # Train classification model
    print("\n2ï¸âƒ£ CLASSIFICATION MODEL (Price Direction)")
    print("=" * 45)
    predictor.load_data(target_type='classification')
    predictor.train_model(model_name='classification_default')
    clf_metrics = predictor.evaluate_model('classification_default')
    predictor.print_performance_report('classification_default')
    
    # Train optimized models
    print("\n3ï¸âƒ£ OPTIMIZED MODELS")
    print("=" * 25)
    
    # Optimized regression
    print("\nğŸ”§ Optimized Regression Model:")
    optimized_reg_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 50,
        'learning_rate': 0.03,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'max_depth': 7,
        'min_data_in_leaf': 20,
        'lambda_l1': 0.1,
        'lambda_l2': 0.1,
        'verbose': -1,
        'random_state': 42
    }
    
    predictor.load_data(target_type='regression')
    predictor.train_model(model_name='regression_optimized', lgb_params=optimized_reg_params)
    predictor.evaluate_model('regression_optimized')
    predictor.print_performance_report('regression_optimized')
    
    # Optimized classification
    print("\nğŸ”§ Optimized Classification Model:")
    optimized_clf_params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 40,
        'learning_rate': 0.03,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'max_depth': 6,
        'min_data_in_leaf': 25,
        'lambda_l1': 0.05,
        'lambda_l2': 0.05,
        'verbose': -1,
        'random_state': 42
    }
    
    predictor.load_data(target_type='classification')
    predictor.train_model(model_name='classification_optimized', lgb_params=optimized_clf_params)
    predictor.evaluate_model('classification_optimized')
    predictor.print_performance_report('classification_optimized')
    
    # Save all models
    print("\n4ï¸âƒ£ SAVING MODELS")
    print("=" * 20)
    for model_name in predictor.models.keys():
        predictor.save_model(model_name)
    
    # Future predictions
    print("\n5ï¸âƒ£ FUTURE PREDICTIONS")
    print("=" * 25)
    
    # Regression predictions
    predictor.load_data(target_type='regression')
    reg_future = predictor.predict_future('regression_optimized', days_ahead=5)
    
    # Classification predictions
    predictor.load_data(target_type='classification')
    clf_future = predictor.predict_future('classification_optimized', days_ahead=5)
    
    # Performance Summary
    print("\n6ï¸âƒ£ OVERALL PERFORMANCE SUMMARY")
    print("=" * 35)
    
    all_summaries = {}
    for model_name in predictor.models.keys():
        if model_name in predictor.model_metrics:
            summary = predictor.get_model_summary(model_name)
            all_summaries[model_name] = summary
    
    # Compare models
    print("\nğŸ“Š MODEL COMPARISON:")
    print("-" * 50)
    
    for model_name, summary in all_summaries.items():
        metrics = summary['performance_metrics']
        if summary['task_type'] == 'regression':
            print(f"{model_name}:")
            print(f"   RMSE: {metrics['test_rmse']:.6f}")
            print(f"   RÂ²: {metrics['test_r2']:.6f}")
            print(f"   Direction Accuracy: {metrics['test_direction_accuracy']:.4f}")
        else:
            print(f"{model_name}:")
            print(f"   Accuracy: {metrics['test_accuracy']:.4f}")
            print(f"   F1-Score: {metrics['test_f1_score']:.4f}")
            print(f"   Precision: {metrics['test_precision']:.4f}")
        print()
    
    print(f"âœ… All models trained and evaluated!")
    print(f"ğŸ“Š Models created: {list(predictor.models.keys())}")
    print(f"ğŸ’¾ Models saved to: {predictor.model_save_dir}")
    
    return predictor


if __name__ == "__main__":
    if not LIGHTGBM_AVAILABLE:
        print("âŒ LightGBM not available. Please install it first:")
        print("   pip install lightgbm scikit-learn")
    else:
        # Example usage
        predictor = create_comprehensive_lgb_models()
