"""
XGBoost Predictive Model for S&P 500 Trading
Uses CSV data generated by the dataset module for training and prediction
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
import json

warnings.filterwarnings('ignore')

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    import xgboost as xgb
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import cross_val_score, TimeSeriesSplit
    XGBOOST_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Some dependencies not available: {e}")
    print("   Install with: pip install xgboost scikit-learn")
    XGBOOST_AVAILABLE = False

class SP500XGBPredictor:
    """
    XGBoost-based predictive model for S&P 500 price movements
    Loads data from CSV files and provides comprehensive ML pipeline
    """
    def __init__(self, csv_dir: str = None, model_save_dir: str = None):
        if csv_dir is None:
            csv_dir = os.path.join(parent_dir, 'data', 'csv')
        if model_save_dir is None:
            model_save_dir = os.path.join(parent_dir, 'models', 'saved_models')
        self.csv_dir = csv_dir
        self.model_save_dir = model_save_dir
        os.makedirs(self.model_save_dir, exist_ok=True)
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        self.target_type = None
        self.models = {}
        self.model_metrics = {}
        self.feature_importance = {}

    def load_data(self, target_type: str = 'regression') -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        train_path = os.path.join(self.csv_dir, 'GSPC_train_2020-01-01_20250807.csv')
        test_path = os.path.join(self.csv_dir, 'GSPC_test_2020-01-01_20250807.csv')
        train_df = pd.read_csv(train_path, index_col=0)
        test_df = pd.read_csv(test_path, index_col=0)
        
        # Use the single 'target' column for both regression and classification
        target_col = 'target'
        self.feature_names = [col for col in train_df.columns if col not in ['date', target_col]]
        self.X_train = train_df[self.feature_names]
        self.X_test = test_df[self.feature_names]
        
        if target_type == 'regression':
            # Use the continuous target values for regression
            self.y_train = train_df[target_col]
            self.y_test = test_df[target_col]
        else:
            # Create binary classification targets (1 for positive returns, 0 for negative)
            self.y_train = (train_df[target_col] > 0).astype(int)
            self.y_test = (test_df[target_col] > 0).astype(int)
            
        self.target_type = target_type
        return self.X_train, self.X_test, self.y_train, self.y_test

    def train_model(self, model_name: str = 'default', xgb_params: Dict = None):
        if self.X_train is None or self.y_train is None:
            raise ValueError('Data not loaded. Call load_data() first!')
        if self.target_type == 'regression':
            if xgb_params is None:
                xgb_params = {'objective': 'reg:squarederror', 'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6}
            model = xgb.XGBRegressor(**xgb_params)
        else:
            if xgb_params is None:
                xgb_params = {'objective': 'binary:logistic', 'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6}
            model = xgb.XGBClassifier(**xgb_params)
        model.fit(self.X_train, self.y_train)
        self.models[model_name] = model
        # Save model
        model_dir = os.path.join(self.model_save_dir, f"xgboost_{self.target_type}_{model_name}")
        os.makedirs(model_dir, exist_ok=True)
        joblib.dump(model, os.path.join(model_dir, 'model.joblib'))
        # Feature importance
        importance = model.feature_importances_
        importance_df = pd.DataFrame({'feature': self.feature_names, 'importance': importance})
        importance_df = importance_df.sort_values('importance', ascending=False)
        importance_df.to_csv(os.path.join(model_dir, 'feature_importance.csv'), index=False)
        self.feature_importance[model_name] = importance_df

    def evaluate_model(self, model_name: str = 'default') -> Dict:
        model = self.models.get(model_name)
        if model is None:
            raise ValueError(f'Model {model_name} not found!')
        y_pred = model.predict(self.X_test)
        metrics = {}
        if self.target_type == 'regression':
            metrics['test_rmse'] = np.sqrt(mean_squared_error(self.y_test, y_pred))
            metrics['test_mae'] = mean_absolute_error(self.y_test, y_pred)
            metrics['test_r2'] = r2_score(self.y_test, y_pred)
        else:
            y_pred_class = (y_pred > 0.5).astype(int) if y_pred.ndim > 1 or y_pred.dtype != int else y_pred
            metrics['test_accuracy'] = accuracy_score(self.y_test, y_pred_class)
            metrics['test_precision'] = precision_score(self.y_test, y_pred_class)
            metrics['test_recall'] = recall_score(self.y_test, y_pred_class)
            metrics['test_f1_score'] = f1_score(self.y_test, y_pred_class)
            cm = confusion_matrix(self.y_test, y_pred_class)
            metrics['confusion_matrix'] = cm.tolist()
        self.model_metrics[model_name] = metrics
        return metrics

    def get_model_summary(self, model_name: str = 'default') -> Dict:
        return {
            'model_name': model_name,
            'task_type': self.target_type,
            'performance_metrics': self.model_metrics.get(model_name, {}),
            'feature_importance': self.feature_importance.get(model_name, pd.DataFrame()),
            'data_info': {
                'features': len(self.feature_names) if self.feature_names else 0,
                'train_samples': len(self.X_train) if self.X_train is not None else 0,
                'test_samples': len(self.X_test) if self.X_test is not None else 0
            }
        }

    def _render_overview_tab(self, models: List[str]) -> None:
        """Render the overview tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ðŸ“ˆ Dataset & Models Overview")
        
        # Dataset info
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Models", len(models))
            st.metric("Training Samples", len(self.X_train) if self.X_train is not None else 0)
        
        with col2:
            st.metric("Test Samples", len(self.X_test) if self.X_test is not None else 0)
            st.metric("Features", len(self.feature_names) if self.feature_names else 0)
        
        with col3:
            if self.X_train is not None:
                train_period = f"{self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}"
                test_period = f"{self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}"
                st.write(f"**Train Period:** {train_period}")
                st.write(f"**Test Period:** {test_period}")
        
        # Models summary
        st.subheader("ðŸ¤– Trained Models")
        models_data = []
        for model_name in models:
            if model_name in self.model_metrics:
                summary = self.get_model_summary(model_name)
                models_data.append({
                    'Model': model_name,
                    'Type': summary['task_type'].title(),
                    'Features': summary['data_info']['features'],
                    'Status': 'âœ… Trained & Evaluated'
                })
            else:
                models_data.append({
                    'Model': model_name,
                    'Type': 'Unknown',
                    'Features': 'N/A',
                    'Status': 'âš ï¸ Trained Only'
                })
        
        st.dataframe(pd.DataFrame(models_data), use_container_width=True)
    
    def _render_performance_tab(self, models: List[str]) -> None:
        """Render the performance metrics tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ðŸŽ¯ Performance Metrics")
        
        selected_model = st.selectbox("Select Model:", models)
        
        if selected_model not in self.model_metrics:
            st.error(f"Model '{selected_model}' not evaluated yet!")
            return
        
        summary = self.get_model_summary(selected_model)
        metrics = summary['performance_metrics']
        
        # Performance metrics display
        if summary['task_type'] == 'regression':
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test RMSE", f"{metrics['test_rmse']:.6f}")
                st.metric("Test MAE", f"{metrics['test_mae']:.6f}")
            
            with col2:
                st.metric("Test RÂ²", f"{metrics['test_r2']:.6f}")
            
            with col3:
                st.write("**XGBoost Model Performance**")
                st.write(f"RMSE: {metrics['test_rmse']:.6f}")
                st.write(f"RÂ²: {metrics['test_r2']:.6f}")
        
        else:  # Classification
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test Accuracy", f"{metrics['test_accuracy']:.4f}")
                st.metric("Test Precision", f"{metrics['test_precision']:.4f}")
            
            with col2:
                st.metric("Test Recall", f"{metrics['test_recall']:.4f}")
                st.metric("Test F1-Score", f"{metrics['test_f1_score']:.4f}")
            
            with col3:
                st.write("**XGBoost Model Performance**")
                st.write(f"Accuracy: {metrics['test_accuracy']:.4f}")
                st.write(f"F1-Score: {metrics['test_f1_score']:.4f}")
            
            # Confusion Matrix
            st.subheader("ðŸ“‹ Confusion Matrix")
            cm = metrics['confusion_matrix']
            if isinstance(cm, list) and len(cm) == 4:
                cm_df = pd.DataFrame([
                    ['True Negative', cm[0]],
                    ['False Positive', cm[1]], 
                    ['False Negative', cm[2]],
                    ['True Positive', cm[3]]
                ], columns=['Type', 'Count'])
                st.dataframe(cm_df, use_container_width=True)
        
        # Feature Importance
        st.subheader("ðŸ” Feature Importance")
        if model_name in self.feature_importance:
            importance_df = self.feature_importance[selected_model]
            if not importance_df.empty:
                st.bar_chart(importance_df.set_index('feature')['importance'].head(10))
                
                with st.expander("ðŸ“Š Full Feature Importance"):
                    st.dataframe(importance_df, use_container_width=True)

    def _render_overview_tab(self, models: List[str]) -> None:
        """Render the overview tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ðŸ“ˆ Dataset & Models Overview")
        
        # Dataset info
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Models", len(models))
            st.metric("Training Samples", len(self.X_train) if self.X_train is not None else 0)
        
        with col2:
            st.metric("Test Samples", len(self.X_test) if self.X_test is not None else 0)
            st.metric("Features", len(self.feature_names) if self.feature_names else 0)
        
        with col3:
            if self.X_train is not None:
                try:
                    train_period = f"{self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}"
                    test_period = f"{self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}"
                    st.write(f"**Train Period:** {train_period}")
                    st.write(f"**Test Period:** {test_period}")
                except:
                    st.write("**Data loaded successfully**")
        
        # Models summary
        st.subheader("ðŸ¤– Trained Models")
        models_data = []
        for model_name in models:
            if model_name in self.model_metrics:
                models_data.append({
                    'Model': model_name,
                    'Type': self.target_type.title(),
                    'Features': len(self.feature_names) if self.feature_names else 'N/A',
                    'Status': 'âœ… Trained & Evaluated'
                })
            else:
                models_data.append({
                    'Model': model_name,
                    'Type': 'Unknown',
                    'Features': 'N/A',
                    'Status': 'âš ï¸ Trained Only'
                })
        
        if models_data:
            models_df = pd.DataFrame(models_data)
            st.dataframe(models_df, use_container_width=True)

    def _render_performance_tab(self, models: List[str]) -> None:
        """Render the performance metrics tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("ðŸŽ¯ Performance Metrics")
        
        selected_model = st.selectbox("Select Model:", models)
        
        if selected_model not in self.model_metrics:
            st.error(f"Model '{selected_model}' not evaluated yet!")
            return
        
        metrics = self.model_metrics[selected_model]
        
        # Performance metrics display
        if self.target_type == 'regression':
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test RMSE", f"{metrics['test_rmse']:.6f}")
                st.metric("Test MAE", f"{metrics['test_mae']:.6f}")
            
            with col2:
                st.metric("Test RÂ²", f"{metrics['test_r2']:.6f}")
            
            with col3:
                st.write("**XGBoost Regression Model**")
                st.write(f"Features: {len(self.feature_names)}")
        
        else:  # Classification
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test Accuracy", f"{metrics['test_accuracy']:.4f}")
                st.metric("Test Precision", f"{metrics['test_precision']:.4f}")
            
            with col2:
                st.metric("Test Recall", f"{metrics['test_recall']:.4f}")
                st.metric("Test F1-Score", f"{metrics['test_f1_score']:.4f}")
            
            with col3:
                st.write("**XGBoost Classification Model**")
                st.write(f"Features: {len(self.feature_names)}")
            
            # Confusion Matrix
            if 'confusion_matrix' in metrics:
                st.subheader("ðŸ“‹ Confusion Matrix")
                cm = metrics['confusion_matrix']
                cm_df = pd.DataFrame([
                    ['True Negative', cm[0][0]],
                    ['False Positive', cm[0][1]],
                    ['False Negative', cm[1][0]],
                    ['True Positive', cm[1][1]]
                ], columns=['Metric', 'Value'])
                
                st.dataframe(cm_df, use_container_width=True)

def create_comprehensive_xgb_models() -> SP500XGBPredictor:
    predictor = SP500XGBPredictor()
    # Regression
    predictor.load_data(target_type='regression')
    predictor.train_model(model_name='optimized')
    predictor.evaluate_model('optimized')
    # Classification
    predictor.load_data(target_type='classification')
    predictor.train_model(model_name='optimized')
    predictor.evaluate_model('optimized')
    return predictor
