"""
Logistic Regression Predictive Model for S&P 500 Trading
Uses CSV data generated by the dataset module for training and prediction
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import joblib
import json

warnings.filterwarnings('ignore')

# Add parent directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.metrics import classification_report, confusion_matrix
    from sklearn.model_selection import cross_val_score, TimeSeriesSplit
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError as e:
    print(f"⚠️  Some dependencies not available: {e}")
    print("   Install with: pip install scikit-learn")
    SKLEARN_AVAILABLE = False

class SP500LogisticRegPredictor:
    """
    Logistic Regression-based predictive model for S&P 500 price movements
    Loads data from CSV files and provides comprehensive ML pipeline
    """
    def __init__(self, csv_dir: str = None, model_save_dir: str = None):
        if csv_dir is None:
            csv_dir = os.path.join(parent_dir, 'data', 'csv')
        if model_save_dir is None:
            model_save_dir = os.path.join(parent_dir, 'models', 'saved_models')
        self.csv_dir = csv_dir
        self.model_save_dir = model_save_dir
        os.makedirs(self.model_save_dir, exist_ok=True)
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        self.target_type = None
        self.models = {}
        self.scalers = {}
        self.model_metrics = {}
        self.feature_importance = {}

    def load_data(self, target_type: str = 'classification') -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        train_path = os.path.join(self.csv_dir, 'GSPC_train_2020-01-01_20250807.csv')
        test_path = os.path.join(self.csv_dir, 'GSPC_test_2020-01-01_20250807.csv')
        train_df = pd.read_csv(train_path, index_col=0)
        test_df = pd.read_csv(test_path, index_col=0)
        
        # Use the single 'target' column for both regression and classification
        target_col = 'target'
        self.feature_names = [col for col in train_df.columns if col not in ['date', target_col]]
        self.X_train = train_df[self.feature_names]
        self.X_test = test_df[self.feature_names]
        
        if target_type == 'regression':
            # Use the continuous target values for regression
            self.y_train = train_df[target_col]
            self.y_test = test_df[target_col]
        else:
            # Create binary classification targets (1 for positive returns, 0 for negative)
            self.y_train = (train_df[target_col] > 0).astype(int)
            self.y_test = (test_df[target_col] > 0).astype(int)
            
        self.target_type = target_type
        return self.X_train, self.X_test, self.y_train, self.y_test

    def train_model(self, model_name: str = 'default', lr_params: Dict = None):
        if self.X_train is None or self.y_train is None:
            raise ValueError('Data not loaded. Call load_data() first!')
        
        # Scale features for logistic regression
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        X_test_scaled = scaler.transform(self.X_test)
        self.scalers[model_name] = scaler
        
        if self.target_type == 'regression':
            # Use Ridge regression for continuous targets
            from sklearn.linear_model import Ridge
            if lr_params is None:
                lr_params = {'alpha': 1.0, 'max_iter': 1000}
            model = Ridge(**lr_params)
        else:
            # Use Logistic Regression for classification
            if lr_params is None:
                lr_params = {'max_iter': 1000, 'random_state': 42}
            model = LogisticRegression(**lr_params)
        
        model.fit(X_train_scaled, self.y_train)
        self.models[model_name] = model
        
        # Save model and scaler
        model_dir = os.path.join(self.model_save_dir, f"logistic_{self.target_type}_{model_name}")
        os.makedirs(model_dir, exist_ok=True)
        joblib.dump(model, os.path.join(model_dir, 'model.joblib'))
        joblib.dump(scaler, os.path.join(model_dir, 'scaler.joblib'))
        
        # Feature importance (coefficients)
        if hasattr(model, 'coef_'):
            if self.target_type == 'classification' and model.coef_.ndim > 1:
                importance = np.abs(model.coef_[0])
            else:
                importance = np.abs(model.coef_)
            importance_df = pd.DataFrame({'feature': self.feature_names, 'importance': importance})
            importance_df = importance_df.sort_values('importance', ascending=False)
            importance_df.to_csv(os.path.join(model_dir, 'feature_importance.csv'), index=False)
            self.feature_importance[model_name] = importance_df

    def evaluate_model(self, model_name: str = 'default') -> Dict:
        model = self.models.get(model_name)
        scaler = self.scalers.get(model_name)
        if model is None or scaler is None:
            raise ValueError(f'Model or scaler {model_name} not found!')
        
        X_test_scaled = scaler.transform(self.X_test)
        y_pred = model.predict(X_test_scaled)
        metrics = {}
        
        if self.target_type == 'regression':
            metrics['test_rmse'] = np.sqrt(mean_squared_error(self.y_test, y_pred))
            metrics['test_mae'] = mean_absolute_error(self.y_test, y_pred)
            metrics['test_r2'] = r2_score(self.y_test, y_pred)
            # Add direction accuracy for regression
            direction_correct = ((self.y_test > 0) == (y_pred > 0)).sum()
            metrics['test_direction_accuracy'] = direction_correct / len(self.y_test)
        else:
            y_pred_class = (y_pred > 0.5).astype(int) if y_pred.dtype == float else y_pred
            metrics['test_accuracy'] = accuracy_score(self.y_test, y_pred_class)
            metrics['test_precision'] = precision_score(self.y_test, y_pred_class)
            metrics['test_recall'] = recall_score(self.y_test, y_pred_class)
            metrics['test_f1_score'] = f1_score(self.y_test, y_pred_class)
            cm = confusion_matrix(self.y_test, y_pred_class)
            metrics['confusion_matrix'] = cm.flatten().tolist()
        
        self.model_metrics[model_name] = metrics
        return metrics

    def get_model_summary(self, model_name: str = 'default') -> Dict:
        return {
            'model_name': model_name,
            'task_type': self.target_type,
            'performance_metrics': self.model_metrics.get(model_name, {}),
            'feature_importance': self.feature_importance.get(model_name, pd.DataFrame()),
            'data_info': {
                'features': len(self.feature_names) if self.feature_names else 0,
                'train_samples': len(self.X_train) if self.X_train is not None else 0,
                'test_samples': len(self.X_test) if self.X_test is not None else 0
            }
        }

    def _render_overview_tab(self, models: List[str]) -> None:
        """Render the overview tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("📈 Dataset & Models Overview")
        
        # Dataset info
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Models", len(models))
            st.metric("Training Samples", len(self.X_train) if self.X_train is not None else 0)
        
        with col2:
            st.metric("Test Samples", len(self.X_test) if self.X_test is not None else 0)
            st.metric("Features", len(self.feature_names) if self.feature_names else 0)
        
        with col3:
            if self.X_train is not None:
                try:
                    # Check if index has datetime methods
                    if hasattr(self.X_train.index, 'strftime') or hasattr(self.X_train.index.min(), 'strftime'):
                        train_period = f"{self.X_train.index.min().strftime('%Y-%m-%d')} to {self.X_train.index.max().strftime('%Y-%m-%d')}"
                        test_period = f"{self.X_test.index.min().strftime('%Y-%m-%d')} to {self.X_test.index.max().strftime('%Y-%m-%d')}"
                        st.write(f"**Train Period:** {train_period}")
                        st.write(f"**Test Period:** {test_period}")
                    else:
                        st.write(f"**Train Samples:** {len(self.X_train)}")
                        st.write(f"**Test Samples:** {len(self.X_test)}")
                except (AttributeError, TypeError):
                    st.write(f"**Train Samples:** {len(self.X_train)}")
                    st.write(f"**Test Samples:** {len(self.X_test)}")
        
        # Models summary
        st.subheader("🤖 Trained Models")
        models_data = []
        for model_name in models:
            if model_name in self.model_metrics:
                summary = self.get_model_summary(model_name)
                models_data.append({
                    'Model': model_name,
                    'Type': summary['task_type'].title(),
                    'Features': summary['data_info']['features'],
                    'Status': '✅ Trained & Evaluated'
                })
            else:
                models_data.append({
                    'Model': model_name,
                    'Type': 'Unknown',
                    'Features': 'N/A',
                    'Status': '⚠️ Trained Only'
                })
        
        st.dataframe(pd.DataFrame(models_data), use_container_width=True)
    
    def _render_performance_tab(self, models: List[str]) -> None:
        """Render the performance metrics tab in Streamlit"""
        try:
            import streamlit as st
        except ImportError:
            print("Streamlit not available for UI rendering")
            return
            
        st.header("🎯 Performance Metrics")
        
        selected_model = st.selectbox("Select Model:", models)
        
        if selected_model not in self.model_metrics:
            st.error(f"Model '{selected_model}' not evaluated yet!")
            return
        
        summary = self.get_model_summary(selected_model)
        metrics = summary['performance_metrics']
        
        # Performance metrics display
        if summary['task_type'] == 'regression':
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test RMSE", f"{metrics['test_rmse']:.6f}")
                st.metric("Test MAE", f"{metrics['test_mae']:.6f}")
            
            with col2:
                st.metric("Test R²", f"{metrics['test_r2']:.6f}")
                if 'test_direction_accuracy' in metrics:
                    st.metric("Direction Accuracy", f"{metrics['test_direction_accuracy']:.4f}")
            
            with col3:
                st.write("**Ridge Regression Performance**")
                st.write(f"RMSE: {metrics['test_rmse']:.6f}")
                st.write(f"R²: {metrics['test_r2']:.6f}")
        
        else:  # Classification
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Test Accuracy", f"{metrics['test_accuracy']:.4f}")
                st.metric("Test Precision", f"{metrics['test_precision']:.4f}")
            
            with col2:
                st.metric("Test Recall", f"{metrics['test_recall']:.4f}")
                st.metric("Test F1-Score", f"{metrics['test_f1_score']:.4f}")
            
            with col3:
                st.write("**Logistic Regression Performance**")
                st.write(f"Accuracy: {metrics['test_accuracy']:.4f}")
                st.write(f"F1-Score: {metrics['test_f1_score']:.4f}")
            
            # Confusion Matrix
            st.subheader("📋 Confusion Matrix")
            cm = metrics['confusion_matrix']
            if isinstance(cm, list) and len(cm) == 4:
                cm_df = pd.DataFrame([
                    ['True Negative', cm[0]],
                    ['False Positive', cm[1]], 
                    ['False Negative', cm[2]],
                    ['True Positive', cm[3]]
                ], columns=['Type', 'Count'])
                st.dataframe(cm_df, use_container_width=True)
        
        # Feature Importance (Coefficients)
        st.subheader("🔍 Feature Importance (Coefficients)")
        if selected_model in self.feature_importance:
            importance_df = self.feature_importance[selected_model]
            if not importance_df.empty:
                st.bar_chart(importance_df.set_index('feature')['importance'].head(10))
                
                with st.expander("📊 Full Feature Importance"):
                    st.dataframe(importance_df, use_container_width=True)

def create_comprehensive_logistic_models() -> SP500LogisticRegPredictor:
    predictor = SP500LogisticRegPredictor()
    
    # Classification (main use case for Logistic Regression)
    predictor.load_data(target_type='classification')
    predictor.train_model(model_name='optimized')
    predictor.evaluate_model('optimized')
    
    # Regression (using Ridge regression)
    predictor.load_data(target_type='regression')
    predictor.train_model(model_name='optimized')
    predictor.evaluate_model('optimized')
    
    return predictor

if __name__ == "__main__":
    if not SKLEARN_AVAILABLE:
        print("❌ scikit-learn not available. Please install it first.")
        sys.exit(1)
    
    print("🚀 Training Logistic Regression Models...")
    predictor = create_comprehensive_logistic_models()
    print("✅ Logistic Regression models trained successfully!")
    
    # Show summary
    for model_name in predictor.models.keys():
        summary = predictor.get_model_summary(model_name)
        print(f"\n📊 Model: {model_name}")
        print(f"   Task: {summary['task_type']}")
        print(f"   Metrics: {summary['performance_metrics']}")
